---
layout: post
title: Machine learning Problem set 3
mathjax: true
---

## Introduction to Machine Learning

## Problem Set 3: Support vector machines

#### Problem 1
(a) Plot the points, and it is easy to find that the classes {+, -} are linearly separable.  
(b) the support vectors are (2, 2), (4, 0), (2, 0), (0, 2).It is easy to find the weight vector w = [1, 1]. The hyperplane is $x_2 + x_1 - 3 = 0$.   
(c) The optimal margin will increase if we remove anyone of (2, 2), (0, 2); it will stay the same if we remove anyone of (4, 0), (0, 2).

#### Problem 2
(a) The margin for $w$, $b$ (i.e., the distance from the hyperplane to the support vector):  
Let $P = (z_1,\cdots, z_n)$ be a point on the hyperplane, and $Q = (x_1,\cdots,x_n)$ be the support vector. The distance $d = |\vec{PQ}|\cos\theta = \frac{|\vec{w}||\vec{PQ}|\cos\theta}{|\vec{w}|} = \frac{|w^T PQ|}{|w|}$ (according to the definition of inner product). Note that, $\vec{PQ}=(x_1-z_1,\cdots, x_n-z_n)$, then, we have $|w^TPQ| = (w_1x_1+\cdots+w_nx_n) - (w_1z_1 + \cdots + w_nz_n)$.  
Because $P$ is a point on the hyperplane:$w^Tz+b=0$ and $Q$ is the support vector which subject to $w^Tx + b = 1$, so the margin can be written as $\frac{1}{|w|}$.  
(b) Let $z$, $d$ be any other separable hyperplane, and let $M$ be its margin, and we have $zx + d > 0$ if $y = 1$, and $zx+d<0$ if $y = -1$. And, the boundary is $zx + d = M$ (from (a)). Since it is a separable hyperplane, it holds $M > 0$ and $|z| > 0$. Dividing $|z|M$ on both side, it also holds that $z'x + d' > 0$ if $y = 1$ and $z'x + d' < 0$ if $y = -1$, where $z' = z/(|z|M)$ and $d' = d/(|z|M)$, and the boundary is $z'x + d' = 1/|z|$, respectively. Thus, it is also a feasible solution for the hard-margin optimization problem. Moreover, from (a), we know that the maximum margin is $1/|w|$, therefore, we have $|w|\leq|z'|$.


(c)From the definition, we have $|z'|^2 = 1/M^2$, since $|w|\leq |z'|$(from (b)), we have $|w|\leq1/M$, therefore, $1/|w|\geq M$.


#### Problem 3
(a) It is a XOR problem which is linearly inseparable.

For linearly separable patterns, the classes of patterns with $n$-dimensional vector $X=(x_1, x_2, \cdots, x_n)$ can be separated with a single decision surface. But from the plot of then given points, we cannot draw a straight line to separate the points $(0, 0)$, $(3, 3)$ from the points $(0, 3)$, $(3, 0)$.

So, the problem is linearly inseparable.

(b) For soft margin SVM problem, the margin optimization problem is defined as:  

$\min_{w, b, \xi} w\cdot w + C\sum_i\xi^{(i)}$

s.t. $(w\cdot x^{(i)} + b)y^{(i)}\geq 1 - \xi^{(i)}$ for all $i$.  
$\xi^{i}\geq 0$ for all $i$.  

So, there is a solution for the problem with soft margin SVM.  
For example:
$w_0 = 0, w_1 = 1, b = 1.5$, $\xi^{(1)} = 0$, $\xi^{(2)} = 1.5$, $\xi^{(3)} = 0$, $\xi^{(4)}=1.5$.

$w_0 = 1, w_1 = 0, b = 1.5, \xi^{(1)} = 0, \xi^{(2)} = 1.5, \xi^{(3)} = 0, \xi^{(4)} = 1.5$

#### Problem 4
(a) It is depend on how we define the feature of the occurance. Let $K(x, z)$ be the unique words that occur in both document $x$ and document $z$.

Given a dictionary $D$ of $n$ words, and a document $x$, if we set the feature function $\phi(x)$ as  
$\phi(w_i) = 1$, if $w_i\in D$ appears in $x$, 0 otherwise;  
then, we can verify that $K(x, z) =\phi(x)\phi(z)$, where $x$ and $z$ are two different documents. In such a case, the $K(x, z)$ is kernel.  
(b) The $K(x,z)$ can be written as $(1+\beta x\cdot z)^2 - 1 = \beta^2(x\cdot z)^2 + 2\beta (x\cdot z)$, and note that $x\cdot z = (x_1z_1 + x_2z_2)$, $K(x, z) = \beta^2x_1^2z_1^2 + \beta^2 x_2^2z_2^2 + 2\beta^2 x_1x_2z_1z_2 + 2\beta x_1z_1 + 2\beta x_2z_2$. If we let $\Phi(x) = (\beta x_1x_1, \beta x_2x_2, \sqrt{2}\beta x_1x_2, \sqrt{2\beta}x_1, \sqrt{2\beta}x_2)$, then $K(x,z) = \Phi(x)\Phi(z)$.  

#### Problem 5
The coding job.

#### Problem 6
The answer is just following Andrew Ng's notes on SVM, page 12.
